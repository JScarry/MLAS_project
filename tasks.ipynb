{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f77676",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "#### This notebook contains 5 tasks supporting the Machine Learning and Statistics module of the Higher Diploma in Science in Data Analytics course at ATU 2023. \n",
    "\n",
    "### 5 task:\n",
    "\n",
    "   [[Task 1]](#ref601) Square roots\n",
    "   \n",
    "   [[Task 2]](#ref602) Chi-square test\n",
    "   \n",
    "   [[Task 3]](#ref603) t-test\n",
    "   \n",
    "   [[Task 4]](#ref604) knn\n",
    "   \n",
    "   [[Task 5]](#ref605) Principal Component Analysis\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba1356f",
   "metadata": {},
   "source": [
    "<a id='ref601'></a>\n",
    "# Task 1\n",
    "\n",
    "Part of assignments for the Machine Learning and Statistics modulel of the Higher Diploma in Science in Data Analytics course at ATU 2023\n",
    "\n",
    "Winter 23/24\n",
    "\n",
    "Author: Jarlath Scarry\n",
    "\n",
    "[Back to top of notebook](#Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400e08c",
   "metadata": {},
   "source": [
    "## Square roots \n",
    "\n",
    "> Square roots are difficult to calculate. In Python, you typically use the power operator (a double asterisk) or a package such as `math`. In this task, you should write a function `sqrt(x)` to approximate the square root of a floating point number x without using the power operator or a package.\n",
    "\n",
    "> Rather, you should use the Newton’s method. Start with an initial guess for the square root called $z0$. You then repeatedly improve it using the following formula, until the difference between some previous guess $zi$ and the next $z{i+1}$ is less than some threshold, say 0.01\n",
    "\n",
    "$$z_{i+1} = z_i - \\frac{z_i  ×  z_i - x}{2z_i}$$\n",
    "\n",
    "[[101]](#ref101) (MACHINE LEARNING AND STATISTICS course material)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e7d46",
   "metadata": {},
   "source": [
    "###  Ways to find square root in python. \n",
    "\n",
    "The easiest way is to use a special operator. One such method is to call the `math` library and use `math.sqrt(x)`. Another is to raise the number to the power of a half. An easy way to do this in python is `x**0.5`. See an example of both of these two methods below.\n",
    "In this exercise we will to do this without special operators but instead using Newtons method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb1e41",
   "metadata": {},
   "source": [
    "### Code examples\n",
    "\n",
    "Eaxmples of finding square root using special operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72447e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x = 16\n",
    "math.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403ad7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b5007",
   "metadata": {},
   "source": [
    "### Coding Newtons method\n",
    "Lets put this formula into code. If we take x as the number and our output should be the Square root of x\n",
    "\n",
    "$$z_{i+1} = z_i - \\frac{z_i  ×  z_i - x}{2z_i}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b5320",
   "metadata": {},
   "source": [
    "### Start with a guess\n",
    "We start by making a guess at the result. Lets take `z = 2` as our initial guess for the result when trying to calculate $\\sqrt{16}$. If we run the code below, it uses our guess and calculates out an extimation of the $\\sqrt{16}$.\n",
    "\n",
    "When we run the code below we get an estimation of the answer. Each time we re-run it we get a better estimation of the result.\n",
    "**If we re-running this piece of code 6 times we have a good result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22311c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number that we want to calculate the square root of \n",
    "x= 16\n",
    "# Our initial guess for the square root. Lets set it as a floating point number, since our result may not be a whole number. \n",
    "z = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newtons method for a better approximation \n",
    "#Each time we run this line of code it calculated a better result. \n",
    "#If we re-running this piece of code 6 times we have a good estimation of the answer.\n",
    "#results:\n",
    "         #1st = 5\n",
    "         #2nd = 4.1\n",
    "         #3rd = 4.001219512195122\n",
    "         #4th = 4.0000001858445895\n",
    "         #5th = 4.000000000000004\n",
    "         #6th = 4\n",
    "z = z -(((z*z)-x)/ (2*z))\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a596167",
   "metadata": {},
   "source": [
    "### Iterative method \n",
    "\n",
    "This is an iterative method. Rather than manually re-running the code 6 times until we get a good result we can set it to itterate a number of times. **Lets code this to loop 10 times which should give a reasonable result in most cases.** The example below shows Ten iterations of the calculation of $\\sqrt{15}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the code into a loop to run 10 times\n",
    "def sqrt(x):\n",
    "    #initial guess for the square root\n",
    "    z = x/4.0\n",
    "    #Loop newtons method 10 times until we get a good approximation.\n",
    "    for i in range(10):\n",
    "        # Newtons method for a better approximation\n",
    "        z = z -(((z*z)-x)/ (2*z))\n",
    "        print(z)\n",
    "        print ((z*z)-x)\n",
    "    # z should now be a good approximation\n",
    "    return z   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ebabc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#call the function and test it on 15. \n",
    "#I also print the results and the calue of ((z*z)-x) which we will look at later.\n",
    "result1 = sqrt(15)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856b3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check pythons value for square root of 15 \n",
    "result2 = 15**0.5\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7eb980",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "After running the code we get a result from the `sqrt(15)` function. $\\sqrt{15}$ is returned as  `3.8729833462074166`. This compares closely to the result when raising to the power of 0.5 method.\n",
    "\n",
    "`sqrt(15)` function result = 3.8729833462074166\n",
    "\n",
    "`15**0.5`           result = 3.872983346207417 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9b378",
   "metadata": {},
   "source": [
    "### Improving the code to be more efficent\n",
    "  \n",
    "#### Negative numbers\n",
    "\n",
    "The square root of negative numbers is undefined. This code will continue to loop if a negative number is input. Any number squared will produce a positive number, so there is no true square root of a negative number. We could improve the code by limiting the input to positive numbers. Then with an `if` return the message \"undefined\" if a negative number is input.\n",
    "\n",
    "#### Iterations \n",
    "\n",
    "This `sqrt()` code is somewhat inefficent. It will repeat the loop 10 times even if a good answer is achieved after the first loop. \n",
    "\n",
    "The calculation  𝑧2−𝑥 is exactly  𝑧𝑒𝑟𝑜 when  𝑧 is the square root of  𝑥. It is greater than zero when  𝑧 is too big. It is less than  𝑧𝑒𝑟𝑜 when  𝑧 is too small. Therfore by using (𝑥2−𝑥) as a cost function, and using it to stop the loop when it approaches zero, we can make the code more efficent. This improvment will stop code when a \"good\" result is achieved, rather than running the 10 loops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt(x):\n",
    "    #initial guess for the square root\n",
    "    z = x/4.0\n",
    "    #Loop while ((z*z)-x) is not very colse to 0\n",
    "    while ((z*z)-x)>1e-10 or ((z*z)-x)< -1e-10:\n",
    "        print(z)\n",
    "        # Newtons method for a better approximation\n",
    "        z = z -(((z*z)-x)/ (2*z))\n",
    "    # z should now be a good approximation\n",
    "    return z  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c145d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#square root of 15. Result is very close to python 15**0.5 method.\n",
    "sqrt(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a8762",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this case calculating the square root of 4 stopped after 6 iterations because we were had achieved a \"good\" result. This reduces the \"cost\" from 10 to 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffa921",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "Ian McLoughlin ATU.ie MLAS lecture notes. For much of the code and inspiration for the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f799645",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[101]<a id='ref101'></a> MACHINE LEARNING AND STATISTICS course material. Task 1 - Square roots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93e3e0",
   "metadata": {},
   "source": [
    "## End task1\n",
    "\n",
    "[Back to top of task](#Task-1)\n",
    "\n",
    "[Back to top of notebook](#Tasks)\n",
    "<hr style=\"border: 2px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c40f5b",
   "metadata": {},
   "source": [
    "<a id='ref602'></a>\n",
    "# Task 2\n",
    "\n",
    "Part of assignments for the Machine Learning and Statistics modulel of the Higher Diploma in Science in Data Analytics course at ATU 2023\n",
    "\n",
    "Winter 23/24\n",
    "\n",
    "Author: Jarlath Scarry\n",
    "\n",
    "[Back to top of notebook](#Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b660ee",
   "metadata": {},
   "source": [
    "### Chi-square test\n",
    "\n",
    "> Consider the below contingency table based on a survey asking respondents whether they prefer coffee or tea and whether they prefer plain or chocolate biscuits. Use scipy.stats to perform a chi-squared test to see whether there is any evidence of an association between drink preference and biscuit preference in this instance.\n",
    "\n",
    "![images/task2_image1.PNG](images/task2_image1.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74227fdc",
   "metadata": {},
   "source": [
    "### Assumptions for Chi-square test\n",
    "\n",
    "When you choose to analyse your data using a chi-square test for independence, you need to make sure that the data you want to analyse \"passes\" two assumptions. You need to do this because it is only appropriate to use a chi-square test for independence if your data passes these two assumptions. If it does not, you cannot use a chi-square test for independence. These two assumptions are:\n",
    "\n",
    "Assumption #1:\n",
    "Your two variables should be measured at an ordinal or nominal level (i.e., categorical data). You can learn more about ordinal and nominal variables in our article: Types of Variable.\n",
    "\n",
    "Assumption #2:\n",
    "Your two variable should consist of two or more categorical, independent groups. Example independent variables that meet this criterion include gender (2 groups: Males and Females), ethnicity (e.g., 3 groups: Caucasian, African American and Hispanic), physical activity level (e.g., 4 groups: sedentary, low, moderate and high), profession (e.g., 5 groups: surgeon, doctor, nurse, dentist, therapist), and so forth. These independent groups should not have any overlap. For example Tea or coffee drinker, Well sometimes I drink Tea, and sometimes Coffee. This is not accepted. The choice should be either or.\n",
    "\n",
    "In the section, Procedure, we illustrate the SPSS Statistics procedure to perform a chi-square test for independence. First, we introduce the example that is used in this guide.\n",
    "\n",
    "[[201]](#ref201) (statistics.laerd.com Chi-Square Test for Association using SPSS Statistics Oct 2023)\n",
    "\n",
    "\n",
    "We have Two variables which are cross tabulated on the table. One variable is Drinks with a choice of Tea or Coffee on the Verticle axis on the Left, and the other Variable is Bisciuts with ca choice of Chocolate or Plain on the horizontal axis at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656f9af",
   "metadata": {},
   "source": [
    "### Chi-Square Tests\n",
    "\n",
    "    \n",
    "### Laerd Statistics Chi-Square Test for Independence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf82ec",
   "metadata": {},
   "source": [
    "### Data for the test\n",
    "Data is generated from the results table above.  It is done in a few steps below using pandas, and generated results are shuffled to make them look more as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy.stats.contingency import crosstab\n",
    "\n",
    "\n",
    "coffee_chocolate = [['Coffee','Chocolate']]*43\n",
    "coffee_plain = [['Coffee','Plain']]*57\n",
    "tea_chocolate = [['Tea','Chocolate']]*56\n",
    "tea_plain = [['Tea','Chocolate']]*56\n",
    "\n",
    "raw_data = coffee_chocolate + coffee_plain + tea_chocolate + tea_plain\n",
    "#shuffle the data\n",
    "random.shuffle(raw_data)\n",
    "# Zip the list - make the rows columns and the columns rows\n",
    "# Interchange the outer and inner lists\n",
    "drink, biscuit = list(zip(*raw_data))  #2 lists, one with deing and one with biscuits.\n",
    "\n",
    "# create a data frame\n",
    "df = pd.DataFrame({'drink': drink, 'biscuit': biscuit})\n",
    "\n",
    "df  # df generated from the table in the question.\n",
    "df.to_csv(r'data\\survey.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c507afb",
   "metadata": {},
   "source": [
    "## Contingency Table\n",
    "\n",
    "Contingency tables are used in statistics to summarize the relationship between several categorical variables. In our example, The Contingency table between the two variables \"Drinks \" and \"Biscuits\" is a Frequency table of these variables presented simultaneously. A chi-squared test conducted on a contingency table can test whether or not a relationship exists between variables. These effects are defined as relationships between rows and columns.\n",
    "\n",
    "[[202]](#ref202) (stackoverflow.com, How to understand the chi square contingency table, Oct 2023)\n",
    "\n",
    "The Contingency Table function produces a table of the joint distribution of two categorical variables. This technique is often used to analyze survey data such as in our small survey.\n",
    "\n",
    "[[203]](#ref203) (scipy.org, scipy.stats.contingency.crosstab, Oct 2023)\n",
    "\n",
    "scipy.stats.contingency.crosstab\n",
    "\n",
    "> This function computes the chi-square statistic and p-value for the hypothesis test of independence of the observed frequencies in the contingency table observed.\n",
    "\n",
    "[[204]](#ref204) (Wikipedia.org, Contingency table, Oct 2023) \n",
    "\n",
    "> The expected frequencies are computed based on the marginal sums under the assumption of independence; see scipy.stats.contingency.expected_freq. The number of degrees of freedom is (expressed using numpy functions and attributes).\n",
    "\n",
    "Crosstab returns a table of counts for each possible unique combination in the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd77be7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cross = pd.crosstab(index=df['drink'], columns=df['biscuit'])\n",
    "#cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18562dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Crosstabs Contingency.\n",
    "\n",
    "cross = ss.contingency.crosstab(df['drink'], df['biscuit'])\n",
    "# Show.\n",
    "cross\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7c356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first, second = cross.elements\n",
    "first, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53601ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "print('SciPy version:', scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['drink'] == first[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6c9d",
   "metadata": {},
   "source": [
    "####  Statistical Test\n",
    "\n",
    "> Chi-square test of independence of variables in a contingency table.\n",
    "\n",
    "[[205]](#ref205) (scipy.org, scipy.stats.chi2_contingency, Oct 2023) \n",
    "\n",
    "> This function computes the chi-square statistic and p-value for the hypothesis test of independence of the observed frequencies in the contingency table observed. The expected frequencies are computed based on the marginal sums under the assumption of independence; see scipy.stats.contingency.expected_freq. The number of degrees of freedom is (expressed using numpy functions and attributes):\n",
    "\n",
    "\n",
    "#### In our case the paramaters are:\n",
    "\n",
    "    The array\n",
    "    \n",
    "    Correction value.  When set to false this prevents the function from applying the Yates correction\n",
    "  \n",
    "> In statistics, Yates's correction for continuity (or Yates's chi-squared test) is used in certain situations when testing for independence in a contingency table. It aims at correcting the error introduced by assuming that the discrete probabilities of frequencies in the table can be approximated by a continuous distribution (chi-squared). In some cases, Yates's correction may adjust too far, and so its current use is limited. \n",
    "\n",
    "[[206]](#ref206) (wikipedia.org, Yates's correction for continuity, Oct 2023) \n",
    "\n",
    "> Like scipy.stats.chisquare, this function computes a chi-square statistic; the convenience this function provides is to figure out the expected frequencies and degrees of freedom from the given contingency table.\n",
    "\n",
    "    dof degrees of freedom.   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c25f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#the contingency table\n",
    "cross.count\n",
    "\n",
    "# Do the statistics. Just do them.\n",
    "Chi2ContingencyResult = ss.chi2_contingency(cross.count, correction=False) ##chi2_contingency is the function that does the chi2 test for independence. \n",
    "\n",
    "# Show.\n",
    "Chi2ContingencyResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535ba5b",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### Expected results?\n",
    "What results would we expect to see? Well we can see the \"expected frequency\" data already on the results table. This can also be quickly calculate this using the `expected_freq` function.  This returns the expected frequencies based on the marginal sums of the table. It shows a results table the same shape as the original table observed. \n",
    "\n",
    "The expected results show what result we should expect to see in a table of independent groups. In this case the expected results can be seen in the table below. For example we would expect to see around 73 people to respond saying they would like Coffee with a Chocolate biscuit.\n",
    "       \n",
    "[[205]](#ref205)(scipy.org, scipy.stats.chi2_contingency, Oct 2023)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51136963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "expected = Chi2ContingencyResult.expected_freq\n",
    "numpy.savetxt(r'data\\expected.csv', expected, delimiter=\",\")\n",
    "title = 'Expected result'\n",
    "expected_df = pd.DataFrame(expected,columns=['Chocolate','Plain'],index=['Coffee', 'Tea'])\n",
    "expected_df = expected_df.style.set_caption(title)\n",
    "expected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624bee86",
   "metadata": {},
   "source": [
    "#### Actual survey results\n",
    "<img src=\"images/task2_image1.PNG\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea95b20",
   "metadata": {},
   "source": [
    "### Actual results\n",
    "\n",
    "The function returns a results  `object` containing the following attributes:\n",
    "\n",
    "  statistic float:  The test statistic. = `statistic=87.31664516129032`\n",
    "\n",
    "  pvalue float: The p-value of the test. = `pvalue=9.24664373812942e-21`\n",
    "\n",
    "  dof int: The degrees of freedom.\n",
    "       This is generally calculated as `dof` = (number of rows - 1) * (number of columns - 1) =`dof=1`\n",
    "\n",
    "  expected_freqndarray, same shape as observed\n",
    "        The expected frequencies, based on the marginal sums of the table.\n",
    "\n",
    "[[205]](#ref205) (scipy.org, scipy.stats.chi2_contingency, Oct 2023) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b567811",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "In the Chi-squared test for independence we are testing if there is a difference in the proportions across different cattegories.\n",
    "\n",
    "Is the choice of a plain or chocolate biscuit dependant on the drink chosen? Does knowing what Drink a person chooses tell us anything about what biscuit the might like? Looking at the data it appears that somone chhosing Tea is more likley to choose a Chocolate biscuit, but we can check this with the Chi-squared test for independence. \n",
    "\n",
    "If we assume at the outset that there is no difference between whether a person likes Tea or Coffee with either Plain or Chocolate biscuits, and take this to be k0, or the NULL hypothesis. If this is true what are the chances that the sample data we have would fit that hypothesis? We need to set the treshold for this hypothesis. If we pass the treshold we reject the NULL hypothesis. \n",
    "\n",
    "So we can take it that a p-value of less than 5% (p-value<0.05) means the result is statistically significant. In otherwords we reject the NULL hypothesis and accept that the sample data is representitive of the overall population. That if a person chooses Tea, they are likley to have a chocolate biscuit. Our p-value is far lower than 5% so we should accept the alternative hypothesis. In other words our assunption that the groups are independent is false based on out survey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfebd1",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    " For a Chi-square test, a p-value that is less than or equal to your significance level indicates there is sufficient evidence to conclude that the observed distribution is not the same as the expected distribution. You can conclude that a relationship exists between the categorical variables. \n",
    " \n",
    " [[207]](#ref207) (statisticsbyjim.com, Chi-Square Test of Independence and an Example, Oct 2023) \n",
    " \n",
    "Our p-value is far lower than 5% so we should accept the alternative hypothesis. In other words our assunption that the groups are independent is false based on out survey.\n",
    "\n",
    "So we should conclude that if somone chooses Tea to drink they are more likley to have a Chocolate biscuit.\n",
    " \n",
    "The p-value is the evidence against a null hypothesis. The smaller the p-value, the strong the evidence that you should reject the null hypothesis. \n",
    "\n",
    "[[208]](#ref208) (stackoverflow.com, how to understand the chi square contingency table, Oct 2023) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a8614e",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "Ian McLoughlin ATU.ie MLAS lecture notes. For much of the code and inspiration for the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fe95f",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[201]<a id='ref201'></a> (statistics.laerd.com Chi-Square Test for Association using SPSS Statistics Oct 2023) https://statistics.laerd.com/spss-tutorials/chi-square-test-for-association-using-spss-statistics.php\n",
    "\n",
    "[202]<a id='ref202'></a> (stackoverflow.com How to understand the chi square contingency table, Oct 2023)\n",
    "https://stackoverflow.com/questions/52692315/how-to-understand-the-chi-square-contingency-table\n",
    "\n",
    "[203]<a id='ref203'></a> (scipy.org, scipy.stats.contingency.crosstab, Oct 2023)\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.contingency.crosstab.html\n",
    "\n",
    "[204]<a id='ref204'></a> (Wikipedia.org, Contingency table, Oct 2023) https://en.wikipedia.org/wiki/Contingency_table \n",
    "\n",
    "[205]<a id='ref205'></a> (scipy.org, scipy.stats.chi2_contingency, Oct 2023) https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html\n",
    "\n",
    "[206]<a id='ref206'></a> (wikipedia.org, Yates's correction for continuity, Oct 2023) https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity\n",
    "\n",
    "[207]<a id='ref207'></a> (statisticsbyjim.com, Chi-Square Test of Independence and an Example, Oct 2023) https://statisticsbyjim.com/hypothesis-testing/chi-square-test-independence-example/#:~:text=For%20a%20Chi%2Dsquare%20test,exists%20between%20the%20categorical%20variables. \n",
    "\n",
    "[208]<a id='ref208'></a> (stackoverflow.com, how to understand the chi square contingency table, Oct 2023) https://stackoverflow.com/questions/52692315/how-to-understand-the-chi-square-contingency-table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a0aa1",
   "metadata": {},
   "source": [
    "## END task2\n",
    "\n",
    "[Back to top of task](#Task-2)\n",
    "\n",
    "[Back to top of notebook](#Tasks)\n",
    "<hr style=\"border: 2px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a0509",
   "metadata": {},
   "source": [
    "<a id='ref603'></a>\n",
    "# Task 3\n",
    "\n",
    "Part of assignments for the Machine Learning and Statistics modulel of the Higher Diploma in Science in Data Analytics course at ATU 2023\n",
    "\n",
    "Winter 23/24\n",
    "\n",
    "Author: Jarlath Scarry\n",
    "\n",
    "[Back to top of notebook](#Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9377da",
   "metadata": {},
   "source": [
    "### Perform a t-test\n",
    "\n",
    ">Perform a t-test on the famous penguins data set to investigate\n",
    "whether there is evidence of a significant difference in the body\n",
    "mass of male and female gentoo penguins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b40c2c",
   "metadata": {},
   "source": [
    "#### t-Tests\n",
    "\n",
    ">A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. \n",
    "**It is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.** It is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known (typically, the scaling term is unknown and is therefore a nuisance parameter). When the scaling term is estimated based on the data, the test statistic—under certain conditions—follows a Student's t distribution. The t-test's most common application is to test whether the means of two populations are different.\n",
    "\n",
    ">History\n",
    "The t-distribution, also known as Student's t-distribution, gets its name from William Sealy Gosset, who first published it in English in 1908 in the scientific journal Biometrika using the pseudonym \"Student\" because his employer preferred staff to use pen names when publishing scientific papers.\n",
    "\n",
    ">Although it was William Gosset after whom the term \"Student\" is penned, it was actually through the work of Ronald Fisher that the distribution became well known as \"Student's distribution\"\n",
    "\n",
    "[[301]](#ref301) (wikipedia.org Student's t-test, Oct 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45c34f",
   "metadata": {},
   "source": [
    "#### What Is a T-Distribution?\n",
    "\n",
    ">The t-distribution, also known as the Student’s t-distribution, is a type of probability distribution that is similar to the normal distribution with its bell. Unlike normal distributions, t- distribution has heavier tails, which result in a greater chance for extreme values.\n",
    "The t-distribution is used in statistics to estimate the significance of population parameters for small sample sizes or unknown variations.\n",
    "The t-distribution is the basis for computing t-tests in statistics.\n",
    "\n",
    "[[303]](#ref303) (Investopedia.com, What Is T-Distribution in Probability? How Do You Use It? Oct 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1bebbd",
   "metadata": {},
   "source": [
    "### Normal distribution\n",
    "\n",
    "Normal distribution is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. \n",
    "In graphical form, the normal distribution appears as a \"bell curve\". \n",
    "In summary then is; The term “Normal Distribution Curve” or “Bell Curve” is used to describe the mathematical \n",
    "concept called normal distribution, sometimes referred to as Gaussian distribution. \n",
    "It refers to the shape that is created when a line is plotted using the data points \n",
    "for an item that meets the criteria of ‘Normal Distribution’. [[2]](#ref2) (Normal distribution, Wikipedia, Nov 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fcd7c5",
   "metadata": {},
   "source": [
    "### Properties of a Normal distribution bell curve and the Area under the curve\n",
    "\n",
    "The area under the graph represents 100% of the data\n",
    "Empirical rule. Data falls within a certain number of standard deviations from the mean  \n",
    "68% =1σ (sigma)\n",
    "95% = 2σ\n",
    "99.7% = 3σ\n",
    "\n",
    "[[315]](#ref315) (Normal Distribution Definition and Properties, Prof. Essa, Nov 2015)\n",
    "\n",
    "There are a few basic properties of a normal distribution.\n",
    "\n",
    "The graph will be Symmetrical.\n",
    "\n",
    "The Mean, Mode and Median are all equal.\n",
    "\n",
    "The standard normal distribution has a mean of 0 and a variance of 1 standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c061d",
   "metadata": {},
   "source": [
    "### Probability density function (PDF)\n",
    "\n",
    "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that sample. \n",
    "\n",
    "So the PDF is used to find the probability of the random variable falling within a range of values, rather than being a specific value. \n",
    "\n",
    "][316]](#ref316) (Probability Density Function, wikipedia, Jan 2023)\n",
    "\n",
    "Lets consider the probability of a womans height being exactly 1.7m?\n",
    "\n",
    "Well if we want to find somone exactly 1.700000m it would be very unlikley, infact we could say that probability is close to Zero. \n",
    "\n",
    "However, what is the probability of finding a woman around 1.7m, say we were willing to accept a height between 1.68 to 1.72m? Then we can calculate the probability based on the area under that part of the bell curve.\n",
    "\n",
    "\n",
    "![image info](https://plus.maths.org/content/sites/plus.maths.org/files/articles/2021/Prob_dist/normal_height.png)\n",
    "\n",
    "[[314]](#ref314) (mathspluss.org, Probability distributions, Oct 2023)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be6c89",
   "metadata": {},
   "source": [
    "#### Laerd statistice t-test\n",
    "\n",
    "The independent-samples t-test (or independent t-test, for short) compares the means between two unrelated groups on the same continuous, dependent variable. \n",
    "\n",
    "For example, you could use an independent t-test to understand whether first year graduate salaries differed based on gender \n",
    "The dependent variable would be \"first year graduate salaries\". \n",
    "Your independent variable would be \"gender\", which has two groups: \"male\" and \"female.\n",
    "\n",
    "[[305]](#ref305) (statistics.laerd.com, Independent t-test using SPSS Statistics, Oct 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae9b6e",
   "metadata": {},
   "source": [
    "**Probability Density Function**\n",
    "\n",
    "$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\big(\\frac{x - \\mu}{\\sigma}\\big)^2 } $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b802e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Data frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Statistics.\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function as written in lectures\n",
    "def normal_pdf(x, mu=0.0, sigma=1.0):\n",
    "  # Answer: A*B.\n",
    "  A = 1.0 / (sigma * np.sqrt(2.0 * np.pi))\n",
    "  B = np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "  return A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ad9b2",
   "metadata": {},
   "source": [
    "#### Plot the normal distribution\n",
    "\n",
    "Lets plot the normal distribution using the formula coded in the `normal_pdf()` function above. Code calculates the probability density based on the formula above.\n",
    "\n",
    "[[306]](#ref306) (MLAS lecture notes, Ian McLoughlin, Oct 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank plot.\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Range of x values.\n",
    "x = np.linspace(-4.0, 4.0, 1001)\n",
    "\n",
    "# Plot the pdf for the standard normal distribution.\n",
    "mu, sigma2 = 0.0, 1.0\n",
    "y = normal_pdf(x, mu=mu, sigma=np.sqrt(sigma2))\n",
    "ax.plot(x, y, label=f\"$\\mu = {mu}, \\sigma^2 = {sigma2}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ec646",
   "metadata": {},
   "source": [
    "Create a function that calculates the PDF for the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02730ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pdf(0)   ##red line. Middle of the standard normal pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pdf(0,0,np.sqrt(0.2))   ##blue line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pdf(0,-2,np.sqrt(0.5))   ##green line... So it looks like we have done a good job of calculating the normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a50d9",
   "metadata": {},
   "source": [
    "#### Different normally distributed curves\n",
    "\n",
    "The below plot examples from wikipedia show normally distributed curves for differing values of $\\mu$ and \t$\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004f663",
   "metadata": {},
   "source": [
    "![Normal Distribution](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1280px-Normal_Distribution_PDF.svg.png)\n",
    "[302](#ref302) (Normal distribution, Wikipedia, Oct 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8ffb2",
   "metadata": {},
   "source": [
    "#### Generating a normal distribution\n",
    "\n",
    "There are many built in functions in Python. Numpy and scipy stats have such functions. Numpy will let us generate normally distributed data using np.random.normal() or normally distributed standard data using np.random.standard_normal. The standard normal distribution is a normal distribution where the mean is 0 and the standard deviation is 1. We can generate a satndard normal distribution with numpy.standard_normal\n",
    "\n",
    "#### Lets plot some Normal distribution examples, first using functions in the Python Numpy library\n",
    "\n",
    "The code below uses Numpy to generate random numbers in a normal distribution.\n",
    "\n",
    "[[307]](#ref307) (W3Schools.com, Normal (Gaussian) Distribution, Oct 2023) Retreived from: \n",
    "\n",
    "[[308]](#ref308) (sharpsightlabs.com, A Quick Introduction to Numpy Random Normal, Joshua Ebner, Oct 2023)\n",
    "\n",
    "3 parameters in the function. These allow us to control the mean, the standard deviation, and the size of the normal distribution\n",
    "\n",
    "I ran a random sample of 100 numbers, mean = 0 and standard deviation of 0.1 and plotted the random results. The data appears to be normally distributed but the bell curve is not well defined due to the small sample size. When the sample size is increased to 10000 we can see a well defined bell curve matching the familiar normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.standard_normal(100000) # Generate random normal data using numpy.random\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6)) # Create an empty plot.\n",
    "\n",
    "ax.hist(random_data, bins=40, density=True) # Plot a histogram of the data.\n",
    "\n",
    "\n",
    "mu, sigma2 = 0.0, 1.0  # Plot the pdf for the standard normal distribution.\n",
    "y = normal_pdf(x, mu=mu, sigma=np.sqrt(sigma2))\n",
    "ax.plot(x, y, label=f'$\\mu = {mu}, \\sigma^2 = {sigma2}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f63f4",
   "metadata": {},
   "source": [
    "### Sampling Distribution\n",
    "\n",
    "Lets look a ittle closer at the random (pseudo-random) numbers generated by numpy. Below code generates 2 arrays each 3 numbers long. The numbers are independantly generated and should have no relationship to eachother, other than that they all fit within a normal sttandard distribution. \n",
    "\n",
    "[[309]](#ref309) (Numpy.org, Random sampling, Oct 2023)\n",
    "\n",
    "The numpy.random module implements pseudo-random number generators (PRNGs or RNGs, for short) with the ability to draw samples from a variety of probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc79d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random normal data.\n",
    "random_data = np.random.standard_normal((10000, 25))\n",
    "\n",
    "# Show.\n",
    "random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean across the rows.\n",
    "random_data.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty figure.\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Histogram of means.\n",
    "ax.hist(random_data.mean(axis=1), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eea50a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty figure.\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Histogram of means.\n",
    "ax.hist(random_data.mean(axis=1), bins=30, density=True)\n",
    "\n",
    "# Plot standard normal distribution.\n",
    "x = np.linspace(-4.0, 4.0, 1001)\n",
    "y = normal_pdf(x)\n",
    "ax.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee344ae",
   "metadata": {},
   "source": [
    "## The Penguin data set.\n",
    "\n",
    "That brings us on to the data for this test. Lets look at the penguins dataset and attempt to perform a t-test to see if there is a relationship between male and female gentoo weights. Lets first load the dataset from seaborn library and have a look at it.\n",
    "\n",
    "[[310]](#ref310) (seaborn.pydata.org, seaborn.load_dataset, Oct 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#penguins = sns.load_dataset('penguins') #load the dataset from seaborn library\n",
    "penguins = pd.read_csv(r\"data\\penguins.csv\")\n",
    "penguins.head(10) #show the first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c28f11",
   "metadata": {},
   "source": [
    "Lets get the penguins by species: Gentoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0ffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gentoos = penguins[penguins['species'] == 'Gentoo']\n",
    "gentoos.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c064d25",
   "metadata": {},
   "source": [
    "The gentoo data info below shows an overview of the data we have for gentoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5894c26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gentoos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3167eec",
   "metadata": {},
   "source": [
    "### Paired groups for ttest\n",
    "\n",
    "\n",
    "We can't measure the weight all the Gentoos in the world, but we do have a sample of them. We can see in the dataset there are a number of catogories including Male/Female, so lets see how many of each and seperate them out. The dataset contains a sample of 61 male gentoos and 58 female gentoos weights. So if we do a calculation on this sample, this statistic can help us to estimate the population parameter. In our case the mean of the sample can be used as an estimate of the mean of the population. So if we seperate them out, this will give us out 2 paired groups (or samples) required for the ttest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae464019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gentoos.sex.unique() #get count of males and females\n",
    "gentoos.value_counts(subset = gentoos.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc39462",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=gentoos, x=\"body_mass_g\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0115a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gentoos_male = gentoos[gentoos['sex'] == 'MALE']\n",
    "gentoos_male.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gentoos_female = gentoos[gentoos['sex'] == 'FEMALE']\n",
    "gentoos_female.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acd741",
   "metadata": {},
   "source": [
    "Now we have the two paired groups, we need to focus on getting the body mass of both groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd981565",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_mass = gentoos_male['body_mass_g']\n",
    "male_mass.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9ea6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "female_mass = gentoos_female['body_mass_g']\n",
    "female_mass.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty plot.\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "# Plot a histogram of the data.\n",
    "\n",
    "plt.hist(female_mass, bins=100,range=(2000,8000))\n",
    "plt.title(\"Female mass\", size=20, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814883bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty plot.\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "# Plot a histogram of the data.\n",
    "\n",
    "plt.hist(male_mass, bins=100,range=(2000,8000))\n",
    "plt.title(\"Male mass\", size=20, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9874d46",
   "metadata": {},
   "source": [
    "### Any evidence of a difference in mass of Male and Female Gentoos?\n",
    "\n",
    "Plot the mass of both male and female gentoos on one histogram with seaborn. We can see there is some overlap, but both sets have seperate distributions. Also I have added the KDE curve (Kernel Density Estimators)\n",
    "\n",
    "From this plot we can say YES \"there is evidence of a significant difference in the body\n",
    "mass of male and female gentoo penguins\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc3260",
   "metadata": {},
   "source": [
    "#### Getting more values from the dataset\n",
    "\n",
    "Getting the mean body mass of both groups. Lets look at the mean of both groups. The mean mass of Female Gentoo Penguins is around 4680g while the mean mass of Male Gentoo Penguins is around 5485g. Here we also find the mean and standard deviation for the group of Gentoos overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fdb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gentoos_mass = gentoos['body_mass_g']\n",
    "gentoos_mean_mass = gentoos_mass.mean()\n",
    "gentoos_sd_mass = gentoos_mass.std()\n",
    "female_mass_mean = female_mass.mean()\n",
    "male_mass_mean = male_mass.mean()\n",
    "female_mass_mean, male_mass_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36039dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.histplot(data=gentoos, x=\"body_mass_g\", hue=\"sex\", bins=100, kde=True)\n",
    "#sns.histplot(data=gentoos, x=\"body_mass_g\", kde=True)\n",
    "\n",
    "plt.xlim(2000, 8000)\n",
    "plt.title(\"Gentoo Penguin mass g\", size=20, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1d620",
   "metadata": {},
   "source": [
    "### Running a ttest on the groups\n",
    "\n",
    "Now we have the data extracted form our data set, lets run a ttest. The two arrays of data we have lend themselves to a ttest.\n",
    "\n",
    "We can use `scipy.stats.ttest_ind()` which will:\n",
    "Calculate the T-test for the means of two independent samples of scores.\n",
    "This is a test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.\n",
    "\n",
    "Perform a t-test on these 2 arrays. The t-test should \n",
    "\n",
    "[[311]](#ref311) (docs.scipy.org, scipy.stats.ttest_ind, Oct 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test.\n",
    "ss.ttest_ind(male_mass, female_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faabbdb",
   "metadata": {},
   "source": [
    "Lets calculate the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom. The df is our sample size -1. `(62+59)-1` = df = 120. We can calculate the quantiles using the scipy Percent point function `ss.t.ppf()`\n",
    "\n",
    "[[312]](#ref312) (Youtube.com, T-test, ANOVA and Chi Squared test made easy, Oct 2023) \n",
    "\n",
    "[[313]](#ref313) (youtube.com, Python for Data Analysis: Hypothesis Testing and T-Tests, DataDaft, Oct 2023) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.t.ppf(q=.025, df=120), ss.t.ppf(q=.975, df=120) #Percent point function. 95% confidence interval for our sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39e4b0",
   "metadata": {},
   "source": [
    "This creates a confidence interval of 95% around our female mass mean. So we see at 95% confidence interval our female_mass  falls within our population mean. This is to be expected as seen in the earlier plots with the overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d67f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "sigma = female_mass_mean/math.sqrt(121) #sample stdev/sample size\n",
    "\n",
    "ss.t.interval(0.95,   #confidence interval\n",
    "              df=120,  #degrees of freedom\n",
    "              loc=gentoos_mean_mass,  #sample mean\n",
    "              scale=sigma) #standard deviation estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa2b89",
   "metadata": {},
   "source": [
    "### ttest Results\n",
    "\n",
    "The scipy ttest returns an object `TtestResult` with the following attributes:\n",
    "    \n",
    "`statisticfloat` or ndarray\n",
    "    The t-statistic.\n",
    "    \n",
    "`pvaluefloat` or ndarray\n",
    "    The p-value associated with the given alternative.\n",
    "    \n",
    "`dffloat` or ndarray\n",
    "    The number of degrees of freedom used in calculation of the t-statistic. This is always NaN for a permutation t-test.   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fb275",
   "metadata": {},
   "source": [
    "### Understanding the results\n",
    "\n",
    "If we assume at the outset that there is no difference between the male and female mass, and take this to be k0, or the NULL hypothesis. If this is true what are the chances that the sample data we have would fit that hypothesis? From what we have seen it looks unlikley, but how to read that from the ttest results?\n",
    "\n",
    "Lets say this is very improbable, we need to set a treshold for this. If we say that the chances of seeing the different mass_means for the pair in our sample and the poplation overall having same mass_mean is very unlikley, could we put a value on that. Lets say there is a 5% chance of this being the case. \n",
    "\n",
    "So we can take it that a p-value of less than 5% (p-value<0.05) means the result is statistically significant. In otherwords we reject the NULL hypothesis and accept that the sample data is representitive of the overall population. That a Gentoo Penguins weight will likley vary depending on their sex. On the plots earlier we can see Males are generally heavier than females.\n",
    "\n",
    "If a p-value reported from a t test is less than 0.05, then that result is said to be statistically significant. If a p-value is greater than 0.05, then the result is insignificant.\n",
    "\n",
    "In this case the pvalue is signifficantly lower than 0.05 which indicates the result is statistically significant. The chance of seeing the sample we have in a population where there is no difference in Male and Female weights in very unlikley.\n",
    "\n",
    "This result (`pvalue=2.133687602018886e-28`) gives us strong enough evidence to reject the NULL hypothesis and say, yes there is likly to be a difference in the weight of a Gentoo penguin based on their sex. \n",
    "\n",
    "The ttest statistic value (`statistic=14.721676481405709`) tells us how much the sample mean deviates from the null hypothesis. I fthe statistic lies outside the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis.\n",
    "\n",
    "So our t-statistic of 14.721676481405709 is well outside our range of -1.979930405052777 to 1.9799304050527766, so this is further evidence to accept the alternative hypothesis and reject the NULL hypothesis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892c947",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this case I would recommend rejecting the NULL hypothesis. I would expect that in the general population a Gentoo Penguins weight will likley vary depending on their sex. On the plots earlier we can see Males are generally heavier than females. The results of the ttest support this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c8bae",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "Ian McLoughlin ATU.ie MLAS lecture notes. For much of the code and inspiration for the rest.\n",
    "\n",
    "Palmer Archipelago (Antarctica) penguin data. For making the penguins dataset available for use. https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data\n",
    "Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa32486",
   "metadata": {},
   "source": [
    "### References\n",
    "[301]<a id='ref301'></a> (title, website.ie, Month Year) Retrieved from: https://en.wikipedia.org/wiki/Student%27s_t-test\n",
    "\n",
    "[302]<a id='ref302'></a> (Normal distribution, Wikipedia, Oct 2023) Retreived from: https://en.wikipedia.org/wiki/Normal_distribution\n",
    "\n",
    "[303]<a id='ref303'></a> (Investopedia.com, What Is T-Distribution in Probability? How Do You Use It? Oct 2023) Retreived from: https://www.investopedia.com/terms/t/tdistribution.asp#toc-what-is-a-t-distribution\n",
    "\n",
    "[304]<a id='ref304'></a> (Normal Distributions (Bell Curve): Definition, Word Problems, Stephanie Glen, NOv 2022) Retrieved from: https://www.statisticshowto.com/probability-and-statistics/normal-distributions/#whatisND\n",
    "  \n",
    "[305]<a id='ref305'></a> (statistics.laerd.com, Independent t-test using SPSS Statistics, Oct 2023) https://statistics.laerd.com/spss-tutorials/independent-t-test-using-spss-statistics.php\n",
    "  \n",
    "[306]<a id='ref306'></a> (MLAS lecture notes, Ian McLoughlin, Oct 2023) Retreived from: ATU.ie lectures\n",
    "\n",
    "[307]<a id='ref307'></a> (W3Schools.com, Normal (Gaussian) Distribution, Oct 2023) Retreived from: https://www.w3schools.com/python/numpy/numpy_random_normal.asp\n",
    "\n",
    "[308]<a id='ref308'></a> (sharpsightlabs.com, A Quick Introduction to Numpy Random Normal, Joshua Ebner, Oct 2023) Retreived from: https://www.sharpsightlabs.com/blog/np-random-randn-explained/\n",
    "\n",
    "[309]<a id='ref309'></a> (Numpy.org, Random sampling, Oct 2023) Retreived from: https://numpy.org/devdocs/reference/random/index.html#quick-start\n",
    "\n",
    "[310]<a id='ref310'></a> (seaborn.pydata.org, seaborn.load_dataset, Oct 2023) Retreived from: https://seaborn.pydata.org/generated/seaborn.load_dataset.html\n",
    "\n",
    "[311]<a id='ref311'></a> (docs.scipy.org, scipy.stats.ttest_ind, Oct 2023) Retreived from:   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "\n",
    "[312]<a id='ref312'></a> (Youtube.com, T-test, ANOVA and Chi Squared test made easy, Global Health with Greg Martin, Oct 2023) Retreived from: https://www.youtube.com/watch?v=ijeEYFnS2v4\n",
    "\n",
    "[313]<a id='ref312'></a> (youtube.com, Python for Data Analysis: Hypothesis Testing and T-Tests, DataDaft, Oct 2023) Retreived from: https://www.youtube.com/watch?v=CIbJSX-biu0\n",
    "\n",
    "[314]<a id='ref314'></a> (mathspluss.org, Probability distributions, Oct 2023) Retreived from: https://plus.maths.org/content/maths-minute-probability-distributions\n",
    "\n",
    "[315]<a id='ref315'></a> (Normal Distribution Definition and Properties, Prof. Essa, Nov 2015) Retrieved from: https://youtu.be/iMak-EW4HtM\n",
    "\n",
    "[316]<a id='ref316'></a> (Probability Density Function, wikipedia, Jan 2023) Retreived from: https://en.wikipedia.org/wiki/Probability_density_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0824e",
   "metadata": {},
   "source": [
    "## END task3\n",
    "\n",
    "[Back to top of task](#Task-3)\n",
    "\n",
    "[Back to top of notebook](#Tasks)\n",
    "<hr style=\"border: 2px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a78d6",
   "metadata": {},
   "source": [
    "Topic 4: k Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb24e15",
   "metadata": {},
   "source": [
    "<a id='ref604'></a>\n",
    "# Task 4\n",
    "\n",
    "Part of assignments for the Machine Learning and Statistics modulel of the Higher Diploma in Science in Data Analytics course at ATU 2023\n",
    "\n",
    "Winter 23/24\n",
    "\n",
    "Author: Jarlath Scarry\n",
    "\n",
    "[Back to top of notebook](#Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd945cb",
   "metadata": {},
   "source": [
    "### k Nearest Neighbours\n",
    "\n",
    ">Using the famous iris data set suggest whether the setosa class is easily separable from the other two classes. Provide evidence for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ea9ca",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "In this exercise we aim to train a machine learning model that can take in feature data from the iris dataset and test the accuracy of the model when it tries to predict which of the three flower varieties a set of features are from.\n",
    "\n",
    "The inputs are recorded measurments of the flowers featurse under the column headings: sepal_length, sepal_width, petal_length and petal_width. The varieties are setosa, versicolor and verginica under the heading class.\n",
    "\n",
    "###  scikit-learn\n",
    "\n",
    "We will use the sikit-learn. Scikit-learn is a machine learning library in Python. It is built on NumPy and SciPy and it contains a wide range of tools for machine learning, including: classification, regression, clustering, and dimension reduction tasks. It is a popular choice for machine learning beginners as it has good documentation and a broad library of functions. It is an excellent platform for learning. The package in python is actually called `sklearn`. This shorter name is helpful as it avoids confusion with extra charachters. It is generally included with annaconda so no need to install it seperatley.\n",
    "\n",
    ">Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.\n",
    "\n",
    "\n",
    "\n",
    "[[401]](#ref401) (scikit-learn.org, Getting Started, Nov 2023)\n",
    "\n",
    "### sikit-learn fit method\n",
    "\n",
    "The scikit-learn `fit()` method is used to train or teach the learning model on a specific dataset. The model begins with default settings that may not be good at predicting the type of flower without any knowledge of flowers. Instead of using its previous knowlodge it learns the patterns in the data and uses those patterns to make predictions on new data. A training step with a fit method is required for most machine learning models before they can be used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae5163",
   "metadata": {},
   "source": [
    "## Start coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60331f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning.\n",
    "import sklearn as sk\n",
    "# Data frames.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports required to run the notebook code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf62aa1",
   "metadata": {},
   "source": [
    "Lets have a look at the data and check for null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924077a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(r\"data\\iris.csv\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24723b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3185011",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755175bf",
   "metadata": {},
   "source": [
    "The data is quite short with just 150 entries but no null (or `NaN`) values. This is good, because we don't need to worry about our results being skewed by empty cells. We can see column headings from the data `head()` output. Varieties are under the heading `class`. The other feature headings can also be seen. We will need these too.\n",
    "\n",
    "To carryout a split train and learn on the data we need separate out the target variable. In this case we are looking to predict class, so we seperate it out as y. The feature data we are using to predict the class is seperated to the variable X. After that we will split the dataset out into training (70%) and testing (30%) sets using the `train_test_split()` function from sikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the target variable. In this case we are looking to predict class, so we seperate it out as y. \n",
    "# The feature data we are using to predict the class is seperated to the variable X. \n",
    "X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = iris['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf1e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "#sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "#Slpit the dataset into training (70%) and testing (30%) sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672acf82",
   "metadata": {},
   "source": [
    "Below using the data `head()` function again we can see the `X_train` and `X_test` sets contain the same data for different selected rows. Also notice the class that we wish to predice is now missing. This is seperated into the `y` datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83318183",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ff266",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is an algorithm that can classify new data points based on the majority class of its k nearest neighbors in the training set. \n",
    "\n",
    "The k-value is an important parameter in KNN. It determines the number of nearest neighbors considered. The best k-value will differ based on the dataset can be optimised by experimentation\n",
    "\n",
    "[[402]](#ref402) (ibm.com, What is the k-nearest neighbors algorithm? Nov 2023)\n",
    "\n",
    ">What is a good value for K in KNN?\n",
    "How to find the optimal value of K in KNN? | by Amey Band ...\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.23 May 2020\n",
    "\n",
    "[[403]](#ref403) (towardsdatasience.com, How to find the optimal value of K in KNN? Nov 2023)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_value = round(math.sqrt((len(iris.index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9fc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and fit a KNeighborsClassifier model\n",
    "knn = KNeighborsClassifier(n_neighbors=knn_value)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1562b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (knn.score(X_test, y_test))*100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3c6d4",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "When the code evaluates on the testing set, it is does so with a mixture of all three classes. The code then uses its knowledge of each class to predict a class. As a result, the accuracy of the code is a measure of its ability to correctly classify all three varieties of flowers, not just Iris setosa.\n",
    "\n",
    "[[404]](#ref404) (towardsdatasience.com, Importance of Distance Metrics in Machine Learning \n",
    "\n",
    "In this case the model can predict a flowers variety or `class` with 97.77% accuracy.\n",
    "\n",
    "\n",
    "### Results for setosa\n",
    "\n",
    "Lets look at the results more closely to see how accuratley setosa can be predicted. For this we can use the sikitlearn `classification_report()` function. It will output a report of our models performance by giving scores for `precision`, `recall`, `F1-score`, and `support` for each class.\n",
    "\n",
    "    Precision is the proportion of positive predictions that are correct\n",
    "\n",
    "    Recall is the proportion of actual positives that are correctly identified\n",
    "\n",
    "    F1-score is a measure of the balance between precision and recall\n",
    "\n",
    "    Support is the number of actual occurrences of each class in the dataset\n",
    "\n",
    "[[405]](#ref405) (kaggle.com/, kNN Classifier Tutorial, Nov 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74297a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@mehtashubh1029/iris-flower-classification-using-knn-1eef6e7f3f84\n",
    "#The classification_report function builds a text report showing the main classification metrics. \n",
    "#https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c07b6",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Using sikit-learn on the iris dataset we predicted a flowers variety or `class` with 97.77% accuracy. More importantly for this exercise we predicted `setosa` with 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d79141",
   "metadata": {},
   "source": [
    "### References\n",
    "[401]<a id='ref401'></a>  (scikit-learn.org, Getting Started, Nov 2023) https://scikit-learn.org/stable/getting_started.html\n",
    "\n",
    "[402]<a id='ref402'></a>  (ibm.com, What is the k-nearest neighbors algorithm? Nov 2023) retreived from: https://www.ibm.com/topics/knn\n",
    "\n",
    "[403]<a id='ref403'></a>  (towardsdatasience.com, How to find the optimal value of K in KNN? Nov 2023) retreived from: https://towardsdatascience.com/how-to-find-the-optimal-value-of-k-in-knn-35d936e554eb\n",
    "\n",
    "[404]<a id='ref404'></a>  (towardsdatasience.com, Importance of Distance Metrics in Machine Learning Modelling, Nov 2023) retreived from:  https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d\n",
    "\n",
    "[405]<a id='ref405'></a>  (kaggle.com/, kNN Classifier Tutorial, Nov 2023) retreived from: https://www.kaggle.com/code/prashant111/knn-classifier-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa405c5",
   "metadata": {},
   "source": [
    "## END task4\n",
    "\n",
    "[Back to top of task](#Task-4)\n",
    "\n",
    "[Back to top of notebook](#Tasks)\n",
    "<hr style=\"border: 2px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa15855",
   "metadata": {},
   "source": [
    "<a id='ref605'></a>\n",
    "# Task 5\n",
    "\n",
    "Part of assignments for the Machine Learning and Statistics modulel of the Higher Diploma in Science in Data Analytics course at ATU 2023\n",
    "\n",
    "Winter 23/24\n",
    "\n",
    "Author: Jarlath Scarry\n",
    "\n",
    "[Back to top of notebook](#Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc88d8",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    ">Perform Principal Component Analysis on the iris data set reducing the number of dimensions to two. Explain the purpose\n",
    "of the analysis and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c066c8",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "The notebook task involves assess the normality of the dataset as a whole and specifically the 'setosa' class using Shapiro-Wilk test. Following this standard scaling is applied to normalize the data features. Finally Principal Component Analysis (PCA) is carried out to reduce the dimensions to two. This allows better visualisation of the variance explained by these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d835248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd                   # Data frames.\n",
    "import sklearn as sk                  # Machine Learning.\n",
    "import sklearn.neighbors as ne        # Nearest neighbors.\n",
    "import sklearn.preprocessing as pre   # Preprocessing.\n",
    "import sklearn.decomposition as dec   # Decomposition.\n",
    "import scipy.stats as ss              # Statistical test.\n",
    "import matplotlib.pyplot as plt       # Plots.\n",
    "import seaborn as sns                 # Statistical plots.\n",
    "import warnings\n",
    "warnings.filterwarnings('error', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641f495",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "df = pd.read_csv(r\"data\\iris.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with NA/Nan.\n",
    "df = df.dropna()\n",
    "\n",
    "# Show.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb06cc0",
   "metadata": {},
   "source": [
    "Separate out the target variable. In this case we are looking to predict class, so we seperate it out as y. \n",
    "\n",
    "The feature data we are using to predict the class is seperated to the variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the target variable. In this case we are looking to predict class, so we seperate it out as y. \n",
    "# The feature data we are using to predict the class is seperated to the variable X. \n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a33ea",
   "metadata": {},
   "source": [
    "## Tests for Normality\n",
    "\n",
    "The approaches can be divided into two main themes: relying on statistical tests or visual inspection. Statistical tests have the advantage of making an objective judgement of normality, but are disadvantaged by sometimes not being sensitive enough at low sample sizes or overly sensitive to large sample sizes. As such, some statisticians prefer to use their experience to make a subjective judgement about the data from plots/graphs. \n",
    "\n",
    "#### Shapiro test\n",
    "\n",
    "The Shapiro (Shapiro-Wilk) test is a statistical test used to test if a sample data is normally distributed or not. \n",
    "The null-hypothesis of this test is that the population is normally distributed. Thus, if the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed. \n",
    "\n",
    " [[502]](#ref502) (wikipedia, Shapiro–Wilk test, Dec 2023)\n",
    "\n",
    "If the Sig. value of the Shapiro-Wilk Test is greater than 0.05, the data is normal. If it is below 0.05, the data significantly deviate from a normal distribution.\n",
    "\n",
    " [[504]](#ref504) (statistics.laerd.com, Testing for Normality using SPSS Statistics, Dec 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97245a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the distribution of the petal_length. Setosa is seperate\n",
    "\n",
    "# Histogram.\n",
    "# Create a histogram for petal length separated by class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=df, x='petal_length', hue='class', kde=True, palette='Set1')\n",
    "plt.title('Distribution of Petal Length')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Count')\n",
    "#plt.legend(title='Class')\n",
    "plt.legend(classes, title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d8ac1",
   "metadata": {},
   "source": [
    "#### Shapiro test result\n",
    "\n",
    "This result indicates that petal length is far from normally distributed across all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.shapiro(df['petal_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b2e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate out gentoos. Lets look at distribution of setosa in isolation\n",
    "df_seto = df[df['class'] == 'setosa']\n",
    "\n",
    "# Histogram.\n",
    "df_seto['petal_length'].hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e611ca",
   "metadata": {},
   "source": [
    "#### Shapiro test result for just setosa\n",
    "\n",
    "The p-value indicates the strength of evidence that the data is normally distributed. With a pvalue >0.05, we can say the 'petal_length' data for 'setosa' may not significantly deviate from a normal distribution. So it looks like within the setosa class, petal_length is likley to be normally distributed. The data for all classes together combined however is not likley to be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def09821",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.shapiro(df_seto['petal_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c70723",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Scaling data for machine learning involves transforming the variable features to a similar scale so they are comparable. It reduces unwanted or disproportionate impact on analyses or machine learning.\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance. We will use Standardscaler to scale the data.\n",
    "\n",
    " [[503]](#ref503) (scikit-learn.org, StandardScaler, Dec 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecea7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the df again\n",
    "# Load the Iris dataset\n",
    "df2 = pd.read_csv(r\"data\\iris.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f916250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the target variable. In this case we are looking to predict class, so we seperate it out as y. \n",
    "# The feature data we are using to predict the class is seperated to the variable X. \n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f246c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard scaler.\n",
    "scaler = pre.StandardScaler()\n",
    "\n",
    "# Show.\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6773e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the data to the scaler.\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833d791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the means and variances.\n",
    "scaler.mean_, scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the above. Does that look right? Yes it matches.\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7665aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed X array.\n",
    "X_transformed = scaler.transform(X)\n",
    "\n",
    "#the earlier fitted scaler is used to transform the \n",
    "#original dataset X into a scaled form (X_transformed) for further analysis. \n",
    "#It keeps the same scaling learned from the original data.\n",
    "\n",
    "# Show.\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means.\n",
    "X_transformed.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0889ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means.\n",
    "X_transformed.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c67fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences squared between first and last row.\n",
    "(X_transformed[0] - X_transformed[-1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original column names.\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685de513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-create data frame.\n",
    "df_X_trans = pd.DataFrame(X_transformed, columns=X.columns)\n",
    "\n",
    "# Show.\n",
    "df_X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95260fd3",
   "metadata": {},
   "source": [
    "## Dimensions\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5da5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the data again.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots and histograms.\n",
    "sns.pairplot(df, hue='class');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e707d",
   "metadata": {},
   "source": [
    "Looking at the pairplots, we can see that setosa are lierarly seperable in a number of pairplots, but there is no way to seperate all classes with a pair of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb0f7c",
   "metadata": {},
   "source": [
    "#### Principal component analysis (PCA) \n",
    "\n",
    "There is no good visualisation for all the variables together. As we have seen, the pair plot is probably the best quick visualisation of all the variables in the iris dataset together. We can see at a glance the spread of points and then easily focus on one pair for a closer look, but is there a better way to combine the data from all the variables? \n",
    "\n",
    "Principal component analysis attempts to do this. It is an attempt to take the key points (or essence) of all the variables and combint them into two bariables that can be easily plotted.\n",
    "\n",
    "PCA is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. \n",
    "\n",
    " [[501]](#ref501) (wikipedia.org, Principal component analysis Dec 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "df = pd.read_csv(r\"data\\iris.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da05560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crete new PCA instance.\n",
    "\n",
    "pca = dec.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d966346",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86478be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit the data to pca\n",
    "pca.fit(X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13085e6",
   "metadata": {},
   "source": [
    "#### Explained variance ratios obtained from performing Principal Component Analysis (PCA)\n",
    "\n",
    "In this case, the `array([0.92461621, 0.05301557])` represents the proportion of variance explained by each principal component. Running the PCA above resulted in two principal components. The variance ratios for these components are approximately 92.46% for the first and 5.30% for the second. This tells us that most of the variance in the orivinal variables is retained by the first principal component. The second retains very little in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty plot.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot scatter plot.\n",
    "ax.plot(X_pca[:, 0], X_pca[:, 1], 'k.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original classifications.\n",
    "df_pca = pd.DataFrame(df[['class']])\n",
    "\n",
    "# Show.\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426869fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate our PCA variables.\n",
    "df_pca['pca0'] = X_pca[:, 0]\n",
    "df_pca['pca1'] = X_pca[:, 1]\n",
    "\n",
    "# Show.\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f0919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pair plot.\n",
    "sns.pairplot(df_pca, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de0fb0",
   "metadata": {},
   "source": [
    "If we look at this plot we can say that KNN will still have have a very difficut time seperating versicolor and verginica. So, it looks like reducing the variables by PCA has not yielded much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7eb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scaled data.\n",
    "df_X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb95e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new PCA instance.\n",
    "pca = dec.PCA(n_components=2)\n",
    "\n",
    "# Fit the scaled data.\n",
    "pca.fit(df_X_trans)\n",
    "\n",
    "# Transform.\n",
    "X_trans_pca = pca.transform(df_X_trans)\n",
    "\n",
    "# Original classifications.\n",
    "df_trans_pca = pd.DataFrame(df[['class']])\n",
    "\n",
    "# Incorporate our PCA variables.\n",
    "df_trans_pca['pca0'] = X_trans_pca[:, 0]\n",
    "df_trans_pca['pca1'] = X_trans_pca[:, 1]\n",
    "\n",
    "# Show.\n",
    "df_trans_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the variance.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad709661",
   "metadata": {},
   "source": [
    "Explained variance ratios obtained from performing Principal Component Analysis (PCA) on the `df_X_trans` data is more balanced. It is about 75% to 25%, compared to 95% to 5% earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe353fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot.\n",
    "sns.pairplot(df_trans_pca, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a4e82",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "After pre processing the data we get a good picture of how difficult it might be to classify the data. This last pair plot would suggest that after initial analysis it will be very difficult to get a clean seperation of versicolor and virginica by machine learning. We have succeeded in reducing the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab34a58",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[501]<a id='ref501'></a> (wikipedia.org, Principal component analysis Dec 2023) Retreived from: https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "[502]<a id='ref502'></a> (wikipedia, Shapiro–Wilk test, Dec 2023) retreived from: https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test\n",
    "\n",
    "[503]<a id='ref503'></a> (scikit-learn.org, StandardScaler, Dec 2023) Retreived from: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "[504]<a id='ref504'></a> (statistics.laerd.com, Testing for Normality using SPSS Statistics, Dec 2023) retreived from: https://statistics.laerd.com/spss-tutorials/testing-for-normality-using-spss-statistics.php\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2dffd",
   "metadata": {},
   "source": [
    "### END tasks\n",
    "\n",
    "[Back to top of task](#Task-5)\n",
    "\n",
    "[Back to top of notebook](#Tasks)\n",
    "<hr style=\"border: 2px solid black\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
